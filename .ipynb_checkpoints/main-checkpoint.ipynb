{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import textwrap\n",
    "\n",
    "# 创建项目文件夹结构 - 修正为平级目录\n",
    "folders = [\n",
    "    'crawler',\n",
    "    'fixtures',  # 测试用的HTML文件\n",
    "    'tests',     # 测试代码文件\n",
    "    'outputs'\n",
    "]\n",
    "\n",
    "for folder in folders:\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    print(f\"创建文件夹: {folder}\")\n",
    "\n",
    "# 创建必要的文件\n",
    "files_to_create = {\n",
    "    'requirements.txt': 'aiohttp==3.9.0\\naiosqlite==0.19.0\\npandas==2.0.3\\nplotly==5.15.0\\nbeautifulsoup4==4.12.2\\nhttpx==0.24.1\\nnest_asyncio==1.5.8',\n",
    "    'crawler/__init__.py': '# 爬虫包初始化',\n",
    "    'README.md': '# Async MiniCrawler\\n\\n基于Jupyter Notebook的异步爬虫项目'\n",
    "}\n",
    "\n",
    "for file_path, content in files_to_create.items():\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"创建文件: {file_path}\")\n",
    "\n",
    "print(\" 项目结构创建完成！\")\n",
    "\n",
    "# 创建测试fixtures - 放在fixtures目录（不是tests/fixtures）\n",
    "test_fixtures = {\n",
    "    'fixtures/quotes_page1.html': '''<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>Quotes to Scrape</title>\n",
    "    <meta charset=\"UTF-8\">\n",
    "</head>\n",
    "<body>\n",
    "    <div class=\"quote\">\n",
    "        <span class=\"text\">\"The best way to predict the future is to invent it.\"</span>\n",
    "        <span>by <small class=\"author\">Alan Kay</small></span>\n",
    "        <div class=\"tags\">\n",
    "            Tags: \n",
    "            <a class=\"tag\" href=\"/tag/future/\">future</a>\n",
    "            <a class=\"tag\" href=\"/tag/invention/\">invention</a>\n",
    "        </div>\n",
    "    </div>\n",
    "    <nav>\n",
    "        <ul class=\"pager\">\n",
    "            <li class=\"next\">\n",
    "                <a href=\"/page/2/\">Next &rarr;</a>\n",
    "            </li>\n",
    "        </ul>\n",
    "    </nav>\n",
    "</body>\n",
    "</html>''',\n",
    "    \n",
    "    'fixtures/books_page1.html': '''<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>All products | Books to Scrape</title>\n",
    "    <meta charset=\"UTF-8\">\n",
    "</head>\n",
    "<body>\n",
    "    <ul class=\"breadcrumb\">\n",
    "        <li><a href=\"index.html\">Home</a></li>\n",
    "        <li class=\"active\">Books</li>\n",
    "    </ul>\n",
    "    \n",
    "    <article class=\"product_pod\">\n",
    "        <div class=\"image_container\">\n",
    "            <a href=\"a-light-in-the-attic_1000/index.html\"><img src=\"media/cache/2c/da/2cdad67c44b002e7ead0cc35693c0e8b.jpg\" alt=\"A Light in the Attic\" class=\"thumbnail\"></a>\n",
    "        </div>\n",
    "        <h3><a href=\"a-light-in-the-attic_1000/index.html\" title=\"A Light in the Attic\">A Light in the Attic</a></h3>\n",
    "        <div class=\"product_price\">\n",
    "            <p class=\"price_color\">&pound;51.77</p>\n",
    "            <p class=\"instock availability\">\n",
    "                <i class=\"icon-ok\"></i>\n",
    "                In stock\n",
    "            </p>\n",
    "        </div>\n",
    "        <p class=\"star-rating Three\">\n",
    "            <i class=\"icon-star\"></i>\n",
    "            <i class=\"icon-star\"></i>\n",
    "            <i class=\"icon-star\"></i>\n",
    "            <i class=\"icon-star\"></i>\n",
    "            <i class=\"icon-star\"></i>\n",
    "        </p>\n",
    "    </article>\n",
    "    \n",
    "    <li class=\"next\">\n",
    "        <a href=\"page-2.html\">next</a>\n",
    "    </li>\n",
    "</body>\n",
    "</html>'''\n",
    "}\n",
    "\n",
    "for file_path, content in test_fixtures.items():\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"创建测试文件: {file_path}\")\n",
    "\n",
    "# 创建CLI命令行接口\n",
    "files_to_create['crawl.py'] = '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Async MiniCrawler 命令行接口\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import argparse\n",
    "from main import run_crawler\n",
    "\n",
    "async def main():\n",
    "    parser = argparse.ArgumentParser(description='Async MiniCrawler - 异步网页爬虫')\n",
    "    subparsers = parser.add_subparsers(dest='command', help='命令')\n",
    "    \n",
    "    # run 命令\n",
    "    run_parser = subparsers.add_parser('run', help='运行爬虫')\n",
    "    run_parser.add_argument('--site', type=str, choices=['quotes', 'books'], \n",
    "                          default='quotes', help='要爬取的站点 (quotes 或 books)')\n",
    "    run_parser.add_argument('--concurrency', type=int, default=5, \n",
    "                          help='并发数 (默认: 5)')\n",
    "    run_parser.add_argument('--max-pages', type=int, default=50, \n",
    "                          help='最大页面数 (默认: 50)')\n",
    "    run_parser.add_argument('--delay', type=float, default=1.0, \n",
    "                          help='请求延迟 (默认: 1.0秒)')\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    if args.command == 'run':\n",
    "        print(f\"开始爬取 {args.site} 站点\")\n",
    "        print(f\"配置: 并发数={args.concurrency}, 最大页面={args.max_pages}, 延迟={args.delay}s\")\n",
    "        \n",
    "        data, stats = await run_crawler(\n",
    "            site=args.site,\n",
    "            concurrency=args.concurrency,\n",
    "            max_pages=args.max_pages,\n",
    "            delay=args.delay\n",
    "        )\n",
    "        \n",
    "        print(f\"爬虫完成! 共收集 {len(data)} 条数据\")\n",
    "        print(f\"统计信息: {stats['successful_pages']} 成功, {stats['failed_pages']} 失败\")\n",
    "        \n",
    "    else:\n",
    "        parser.print_help()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    asyncio.run(main())\n",
    "'''\n",
    "\n",
    "# 创建测试文件 - 修正路径引用\n",
    "test_files = {\n",
    "    'tests/test_crawler.py': '''#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "爬虫测试文件 - 测试解析功能\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# 修正路径导入\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "\n",
    "try:\n",
    "    from main import PageParser, AsyncCrawler\n",
    "except ImportError as e:\n",
    "    print(f\"导入错误: {e}\")\n",
    "    # 在Jupyter中运行时可能需要重新定义类\n",
    "    print(\"在Jupyter环境中运行测试...\")\n",
    "\n",
    "def test_parse_quotes():\n",
    "    \"\"\"测试名言页面解析\"\"\"\n",
    "    print(\"测试名言页面解析...\")\n",
    "    \n",
    "    # 使用绝对路径确保能找到文件（现在fixtures在项目根目录）\n",
    "    fixture_path = os.path.join(parent_dir, 'fixtures', 'quotes_page1.html')\n",
    "    \n",
    "    with open(fixture_path, 'r', encoding='utf-8') as f:\n",
    "        html = f.read()\n",
    "    \n",
    "    parser = PageParser()\n",
    "    quotes, next_url = parser.parse_quotes(html, 'http://test.com')\n",
    "    \n",
    "    # 验证解析结果\n",
    "    assert len(quotes) == 1, f\"应该解析出1条名言，实际得到: {len(quotes)}\"\n",
    "    assert quotes[0]['author'] == 'Alan Kay', f\"作者应该是Alan Kay，实际是: {quotes[0]['author']}\"\n",
    "    assert 'future' in quotes[0]['tags'], f\"应该包含future标签，实际标签: {quotes[0]['tags']}\"\n",
    "    assert 'invention' in quotes[0]['tags'], f\"应该包含invention标签，实际标签: {quotes[0]['tags']}\"\n",
    "    assert next_url == 'http://test.com/page/2/', f\"下一页URL应该是http://test.com/page/2/，实际是: {next_url}\"\n",
    "    \n",
    "    # 验证名言文本\n",
    "    expected_text = '\"The best way to predict the future is to invent it.\"'\n",
    "    assert quotes[0]['text'] == expected_text, f\"名言文本不匹配，期望: {expected_text}，实际: {quotes[0]['text']}\"\n",
    "    \n",
    "    print(\"✓ 名言解析测试通过\")\n",
    "    return quotes\n",
    "\n",
    "def test_parse_books():\n",
    "    \"\"\"测试图书页面解析\"\"\"\n",
    "    print(\"测试图书页面解析...\")\n",
    "    \n",
    "    # 使用绝对路径确保能找到文件（现在fixtures在项目根目录）\n",
    "    fixture_path = os.path.join(parent_dir, 'fixtures', 'books_page1.html')\n",
    "    \n",
    "    with open(fixture_path, 'r', encoding='utf-8') as f:\n",
    "        html = f.read()\n",
    "    \n",
    "    parser = PageParser()\n",
    "    books, next_url = parser.parse_books(html, 'http://test.com')\n",
    "    \n",
    "    # 验证解析结果\n",
    "    assert len(books) == 1, f\"应该解析出1本图书，实际得到: {len(books)}\"\n",
    "    assert books[0]['title'] == 'A Light in the Attic', f\"书名应该是A Light in the Attic，实际是: {books[0]['title']}\"\n",
    "    assert books[0]['price'] == '£51.77', f\"价格应该是£51.77，实际是: {books[0]['price']}\"\n",
    "    assert books[0]['stock'] == 'In stock', f\"库存状态应该是In stock，实际是: {books[0]['stock']}\"\n",
    "    assert books[0]['rating'] == 'Three', f\"评分应该是Three，实际是: {books[0]['rating']}\"\n",
    "    assert next_url == 'http://test.com/page-2.html', f\"下一页URL应该是http://test.com/page-2.html，实际是: {next_url}\"\n",
    "    \n",
    "    print(\"✓ 图书解析测试通过\")\n",
    "    return books\n",
    "\n",
    "def test_parse_empty_html():\n",
    "    \"\"\"测试空HTML解析\"\"\"\n",
    "    print(\"测试空HTML解析...\")\n",
    "    \n",
    "    parser = PageParser()\n",
    "    \n",
    "    # 测试空HTML\n",
    "    quotes, next_url = parser.parse_quotes('', 'http://test.com')\n",
    "    assert len(quotes) == 0, \"空HTML应该返回0条名言\"\n",
    "    assert next_url is None, \"空HTML应该返回None下一页URL\"\n",
    "    \n",
    "    books, next_url = parser.parse_books('', 'http://test.com')\n",
    "    assert len(books) == 0, \"空HTML应该返回0本图书\"\n",
    "    assert next_url is None, \"空HTML应该返回None下一页URL\"\n",
    "    \n",
    "    print(\"✓ 空HTML解析测试通过\")\n",
    "\n",
    "def test_parse_invalid_html():\n",
    "    \"\"\"测试无效HTML解析\"\"\"\n",
    "    print(\"测试无效HTML解析...\")\n",
    "    \n",
    "    parser = PageParser()\n",
    "    \n",
    "    # 测试无效HTML\n",
    "    invalid_html = '<div>Invalid HTML content</div>'\n",
    "    quotes, next_url = parser.parse_quotes(invalid_html, 'http://test.com')\n",
    "    assert len(quotes) == 0, \"无效HTML应该返回0条名言\"\n",
    "    assert next_url is None, \"无效HTML应该返回None下一页URL\"\n",
    "    \n",
    "    books, next_url = parser.parse_books(invalid_html, 'http://test.com')\n",
    "    assert len(books) == 0, \"无效HTML应该返回0本图书\"\n",
    "    assert next_url is None, \"无效HTML应该返回None下一页URL\"\n",
    "    \n",
    "    print(\"✓ 无效HTML解析测试通过\")\n",
    "\n",
    "def test_parse_html_with_missing_elements():\n",
    "    \"\"\"测试缺失元素的HTML解析\"\"\"\n",
    "    print(\"测试缺失元素的HTML解析...\")\n",
    "    \n",
    "    parser = PageParser()\n",
    "    \n",
    "    # 测试缺失重要元素的HTML\n",
    "    partial_html = ('<div class=\"quote\">'\n",
    "                   '<span class=\"text\">\"Test quote\"</span>'\n",
    "                   '<!-- 故意缺少author元素 -->'\n",
    "                   '</div>')\n",
    "    \n",
    "    quotes, next_url = parser.parse_quotes(partial_html, 'http://test.com')\n",
    "    assert len(quotes) == 0, \"缺失author元素应该返回0条名言\"\n",
    "    \n",
    "    print(\"✓ 缺失元素解析测试通过\")\n",
    "\n",
    "async def test_crawler_initialization():\n",
    "    \"\"\"测试爬虫初始化\"\"\"\n",
    "    print(\"测试爬虫初始化...\")\n",
    "    \n",
    "    # 测试quotes爬虫初始化\n",
    "    quotes_crawler = AsyncCrawler('quotes', concurrency=2, max_pages=5, delay=1.0)\n",
    "    assert quotes_crawler.site == 'quotes'\n",
    "    assert quotes_crawler.base_url == 'https://quotes.toscrape.com'\n",
    "    assert quotes_crawler.concurrency == 2\n",
    "    assert quotes_crawler.max_pages == 5\n",
    "    assert quotes_crawler.delay == 1.0\n",
    "    \n",
    "    # 测试books爬虫初始化\n",
    "    books_crawler = AsyncCrawler('books', concurrency=3, max_pages=10, delay=2.0)\n",
    "    assert books_crawler.site == 'books'\n",
    "    assert books_crawler.base_url == 'https://books.toscrape.com'\n",
    "    assert books_crawler.concurrency == 3\n",
    "    assert books_crawler.max_pages == 10\n",
    "    assert books_crawler.delay == 2.0\n",
    "    \n",
    "    print(\"✓ 爬虫初始化测试通过\")\n",
    "\n",
    "def simple_test():\n",
    "    \"\"\"简化测试 - 用于在Notebook中直接运行\"\"\"\n",
    "    print(\"运行简化测试...\")\n",
    "    try:\n",
    "        test_parse_quotes()\n",
    "        test_parse_books()\n",
    "        print(\" 基本解析测试通过\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\" 测试失败: {e}\")\n",
    "        return False\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(\"开始运行爬虫测试...\\\\n\")\n",
    "    \n",
    "    try:\n",
    "        test_parse_quotes()\n",
    "        test_parse_books()\n",
    "        test_parse_empty_html()\n",
    "        test_parse_invalid_html()\n",
    "        test_parse_html_with_missing_elements()\n",
    "        asyncio.run(test_crawler_initialization())\n",
    "        \n",
    "        print(\"\\\\n 所有测试通过!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\\\n 测试失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        sys.exit(1)\n",
    "''',\n",
    "    \n",
    "    'tests/__init__.py': '# 测试包初始化',\n",
    "    \n",
    "    'tests/conftest.py': '''\"\"\"\n",
    "Pytest 配置文件\n",
    "\"\"\"\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 添加项目根目录到Python路径\n",
    "current_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "parent_dir = os.path.dirname(current_dir)\n",
    "if parent_dir not in sys.path:\n",
    "    sys.path.insert(0, parent_dir)\n",
    "'''\n",
    "}\n",
    "\n",
    "# 创建测试文件\n",
    "for file_path, content in test_files.items():\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(content)\n",
    "    print(f\"创建测试文件: {file_path}\")\n",
    "\n",
    "# 创建crawl.py文件\n",
    "crawl_py_content = files_to_create['crawl.py']\n",
    "with open('crawl.py', 'w', encoding='utf-8') as f:\n",
    "    f.write(crawl_py_content)\n",
    "print(f\"创建文件: crawl.py\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: aiohttp in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (3.8.6)\n",
      "Requirement already satisfied: aiosqlite in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: pandas in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (1.3.5)\n",
      "Requirement already satisfied: plotly in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (5.18.0)\n",
      "Requirement already satisfied: beautifulsoup4 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (4.13.5)\n",
      "Requirement already satisfied: httpx in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (0.24.1)\n",
      "Requirement already satisfied: nest_asyncio in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (1.5.6)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (1.3.3)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (2.1.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (1.9.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (24.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (4.7.1)\n",
      "Requirement already satisfied: asynctest==0.13.0 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (0.13.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (4.0.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (6.0.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: packaging in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from plotly) (22.0)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from plotly) (8.2.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from beautifulsoup4) (2.4.1)\n",
      "Requirement already satisfied: idna in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from httpx) (3.10)\n",
      "Requirement already satisfied: httpcore<0.18.0,>=0.15.0 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from httpx) (0.17.3)\n",
      "Requirement already satisfied: sniffio in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from httpx) (1.3.1)\n",
      "Requirement already satisfied: certifi in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from httpx) (2022.12.7)\n",
      "Requirement already satisfied: importlib-metadata in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from attrs>=17.3.0->aiohttp) (4.11.3)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from httpcore<0.18.0,>=0.15.0->httpx) (0.14.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from httpcore<0.18.0,>=0.15.0->httpx) (3.7.1)\n",
      "Requirement already satisfied: six>=1.5 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.16.0)\n",
      "Requirement already satisfied: exceptiongroup in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from anyio<5.0,>=3.0->httpcore<0.18.0,>=0.15.0->httpx) (1.3.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from importlib-metadata->attrs>=17.3.0->aiohttp) (3.11.0)\n",
      "Using pip 22.3.1 from D:\\Anaconda3_\\envs\\pytorch\\lib\\site-packages\\pip (python 3.7)\n",
      "Requirement already satisfied: aiohttp in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (3.8.6)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (4.7.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (4.0.3)\n",
      "Requirement already satisfied: asynctest==0.13.0 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (0.13.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (1.3.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (24.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (2.1.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (1.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (1.9.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from aiohttp) (6.0.5)\n",
      "Requirement already satisfied: importlib-metadata in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from attrs>=17.3.0->aiohttp) (4.11.3)\n",
      "Requirement already satisfied: idna>=2.0 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from yarl<2.0,>=1.0->aiohttp) (3.10)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\anaconda3_\\envs\\pytorch\\lib\\site-packages (from importlib-metadata->attrs>=17.3.0->aiohttp) (3.11.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install aiohttp aiosqlite pandas plotly beautifulsoup4 httpx nest_asyncio -i https://pypi.tuna.tsinghua.edu.cn/simple --trusted-host pypi.tuna.tsinghua.edu.cn\n",
    "!pip install aiohttp --verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "所有库导入成功\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import aiohttp\n",
    "import aiosqlite\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin, urlparse\n",
    "import hashlib\n",
    "import json\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import time\n",
    "import nest_asyncio\n",
    "from urllib.robotparser import RobotFileParser\n",
    "import os\n",
    "\n",
    "nest_asyncio.apply()\n",
    "print(\"所有库导入成功\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#URL数据库类\n",
    "class URLDatabase:\n",
    "    def __init__(self, db_path=':memory:'):\n",
    "        self.db_path = db_path\n",
    "        self.conn = None\n",
    "        \n",
    "    async def init_db(self):\n",
    "        self.conn = await aiosqlite.connect(self.db_path)\n",
    "        # 先删除旧表（如果存在），确保表结构正确\n",
    "        await self.conn.execute('DROP TABLE IF EXISTS seen_urls')\n",
    "        await self.conn.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS seen_urls (\n",
    "                url TEXT PRIMARY KEY,\n",
    "                status TEXT,\n",
    "                last_seen TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                content_hash TEXT\n",
    "            )\n",
    "        ''')\n",
    "        await self.conn.commit()\n",
    "        print(\"数据库初始化完成\")\n",
    "    \n",
    "    async def is_seen(self, url):\n",
    "        async with self.conn.execute(\"SELECT url FROM seen_urls WHERE url = ?\", (url,)) as cursor:\n",
    "            result = await cursor.fetchone()\n",
    "            return result is not None\n",
    "    \n",
    "    async def mark_seen(self, url, status, content_hash=None):\n",
    "        await self.conn.execute('''\n",
    "            INSERT OR REPLACE INTO seen_urls (url, status, content_hash) \n",
    "            VALUES (?, ?, ?)\n",
    "        ''', (url, status, content_hash))\n",
    "        await self.conn.commit()\n",
    "    \n",
    "    async def close(self):\n",
    "        if self.conn:\n",
    "            await self.conn.close()\n",
    "            print(\"数据库连接已关闭。\")\n",
    "        else:\n",
    "            # 就是这里！当 self.conn 为 None (即没有活跃连接) 时，会打印这句话\n",
    "            print(\"尝试关闭数据库连接，但没有激活的连接。\") # 可以通过这个打印来确认"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobotsChecker:\n",
    "    def __init__(self):\n",
    "        self.robot_parsers = {}\n",
    "    \n",
    "    async def can_fetch(self, url, user_agent='*'):\n",
    "        \"\"\"检查是否允许爬取\"\"\"\n",
    "        parsed_url = urlparse(url)\n",
    "        base_url = f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "        \n",
    "        if base_url not in self.robot_parsers:\n",
    "            # 获取robots.txt\n",
    "            robot_url = f\"{base_url}/robots.txt\"\n",
    "            try:\n",
    "                async with aiohttp.ClientSession() as session:\n",
    "                    async with session.get(robot_url, timeout=10) as response:\n",
    "                        if response.status == 200:\n",
    "                            robot_content = await response.text()\n",
    "                            parser = RobotFileParser()\n",
    "                            parser.parse(robot_content.splitlines())\n",
    "                            self.robot_parsers[base_url] = parser\n",
    "                        else:\n",
    "                            # 如果没有robots.txt，默认允许\n",
    "                            self.robot_parsers[base_url] = None\n",
    "            except:\n",
    "                self.robot_parsers[base_url] = None\n",
    "        \n",
    "        parser = self.robot_parsers[base_url]\n",
    "        return parser is None or parser.can_fetch(user_agent, url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PageParser:\n",
    "    @staticmethod\n",
    "    def parse_quotes(html, page_url):\n",
    "        \"\"\"解析Quotes页面\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        quotes = []\n",
    "        \n",
    "        for quote_div in soup.find_all('div', class_='quote'):\n",
    "            text_elem = quote_div.find('span', class_='text')\n",
    "            author_elem = quote_div.find('small', class_='author')\n",
    "            \n",
    "            if text_elem and author_elem:\n",
    "                text = text_elem.get_text(strip=True)\n",
    "                author = author_elem.get_text(strip=True)\n",
    "                \n",
    "                # 获取作者链接\n",
    "                author_link = quote_div.find('a', href=True)\n",
    "                author_url = urljoin(page_url, author_link['href']) if author_link else None\n",
    "                \n",
    "                # 获取标签\n",
    "                tags = []\n",
    "                tags_container = quote_div.find('div', class_='tags')\n",
    "                if tags_container:\n",
    "                    tags = [tag.get_text(strip=True) for tag in tags_container.find_all('a', class_='tag')]\n",
    "                \n",
    "                quotes.append({\n",
    "                    'text': text,\n",
    "                    'author': author,\n",
    "                    'tags': tags,\n",
    "                    'author_url': author_url,\n",
    "                    'page_url': page_url\n",
    "                })\n",
    "        \n",
    "        # 查找下一页链接\n",
    "        next_link = soup.find('li', class_='next')\n",
    "        next_url = urljoin(page_url, next_link.find('a')['href']) if next_link and next_link.find('a') else None\n",
    "        \n",
    "        return quotes, next_url\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_books(html, page_url):\n",
    "        \"\"\"解析Books页面\"\"\"\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        books = []\n",
    "        \n",
    "        for book in soup.find_all('article', class_='product_pod'):\n",
    "            # 获取标题\n",
    "            title_elem = book.find('h3').find('a') if book.find('h3') else None\n",
    "            title = title_elem['title'] if title_elem and 'title' in title_elem.attrs else 'Unknown'\n",
    "            \n",
    "            # 获取价格\n",
    "            price_elem = book.find('p', class_='price_color')\n",
    "            price = price_elem.get_text(strip=True) if price_elem else 'Unknown'\n",
    "            \n",
    "            # 获取库存状态\n",
    "            stock_elem = book.find('p', class_='instock')\n",
    "            stock = stock_elem.get_text(strip=True) if stock_elem else 'Unknown'\n",
    "            \n",
    "            # 获取评分\n",
    "            rating_elem = book.find('p', class_='star-rating')\n",
    "            rating = rating_elem['class'][1] if rating_elem and len(rating_elem.get('class', [])) > 1 else 'None'\n",
    "            \n",
    "            # 获取分类（从面包屑导航）\n",
    "            category = 'Unknown'\n",
    "            breadcrumb = soup.find('ul', class_='breadcrumb')\n",
    "            if breadcrumb:\n",
    "                category_items = breadcrumb.find_all('li')\n",
    "                if len(category_items) >= 2:\n",
    "                    category = category_items[-2].get_text(strip=True)\n",
    "            \n",
    "            # 获取产品链接\n",
    "            product_link = title_elem['href'] if title_elem and 'href' in title_elem.attrs else None\n",
    "            product_url = urljoin(page_url, product_link) if product_link else None\n",
    "            \n",
    "            books.append({\n",
    "                'title': title,\n",
    "                'price': price,\n",
    "                'stock': stock,\n",
    "                'rating': rating,\n",
    "                'category': category,\n",
    "                'product_url': product_url,\n",
    "                'page_url': page_url\n",
    "            })\n",
    "        \n",
    "        # 查找下一页链接\n",
    "        next_link = soup.find('li', class_='next')\n",
    "        next_url = urljoin(page_url, next_link.find('a')['href']) if next_link and next_link.find('a') else None\n",
    "        \n",
    "        return books, next_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AsyncCrawler:\n",
    "    def __init__(self, site, concurrency=5, max_pages=50, delay=1.0):\n",
    "        self.site = site\n",
    "        self.base_url = \"https://quotes.toscrape.com\" if site == \"quotes\" else \"https://books.toscrape.com\"\n",
    "        self.concurrency = concurrency\n",
    "        self.max_pages = max_pages\n",
    "        self.delay = delay\n",
    "        self.semaphore = asyncio.Semaphore(concurrency)\n",
    "        \n",
    "        self.db = URLDatabase()\n",
    "        self.robots_checker = RobotsChecker()\n",
    "        self.parser = PageParser()\n",
    "        \n",
    "        self.collected_data = []\n",
    "        self.stats = {\n",
    "            'total_pages': 0,\n",
    "            'successful_pages': 0,\n",
    "            'failed_pages': 0,\n",
    "            'duplicate_pages': 0,\n",
    "            'start_time': None,\n",
    "            'end_time': None\n",
    "        }\n",
    "    \n",
    "    async def fetch_url(self, session, url, retries=3):\n",
    "        for attempt in range(retries):\n",
    "            try:\n",
    "                async with self.semaphore:\n",
    "                    await asyncio.sleep(self.delay)\n",
    "\n",
    "                    # 直接使用传入的 session 参数\n",
    "                    async with session.get(url, timeout=30) as response:\n",
    "                        if response.status == 200:\n",
    "                            content = await response.text()\n",
    "                            content_hash = hashlib.md5(content.encode()).hexdigest()\n",
    "                            return content, content_hash, True\n",
    "                        else:\n",
    "                            print(f\"尝试 {attempt + 1}: {url} 返回状态码 {response.status}\")\n",
    "            except Exception as e:\n",
    "                print(f\"尝试 {attempt + 1}: {url} 错误: {e}\")\n",
    "\n",
    "            await asyncio.sleep(2 ** attempt)\n",
    "\n",
    "        return None, None, False\n",
    "    \n",
    "    async def process_page(self, session, url):\n",
    "        \"\"\"处理单个页面\"\"\"\n",
    "        try:\n",
    "            self.stats['total_pages'] += 1\n",
    "        \n",
    "            # 检查robots.txt\n",
    "            if not await self.robots_checker.can_fetch(url):\n",
    "                print(f\"由于robots.txt限制，跳过: {url}\")\n",
    "                return None\n",
    "        \n",
    "            # 检查是否已访问\n",
    "            if await self.db.is_seen(url):\n",
    "                print(f\"跳过已访问URL: {url}\")\n",
    "                self.stats['duplicate_pages'] += 1\n",
    "                return None\n",
    "        \n",
    "            # 抓取页面\n",
    "            content, content_hash, success = await self.fetch_url(session, url)\n",
    "        \n",
    "            if not success:\n",
    "                print(f\"抓取失败: {url}\")\n",
    "                await self.db.mark_seen(url, \"failed\", content_hash)\n",
    "                self.stats['failed_pages'] += 1\n",
    "                return None\n",
    "        \n",
    "            # 解析页面\n",
    "            if self.site == \"quotes\":\n",
    "                data, next_url = self.parser.parse_quotes(content, url)\n",
    "            else:\n",
    "                data, next_url = self.parser.parse_books(content, url)\n",
    "        \n",
    "            # 保存数据\n",
    "            self.collected_data.extend(data)\n",
    "        \n",
    "            # 标记为成功访问\n",
    "            await self.db.mark_seen(url, \"success\", content_hash)\n",
    "            self.stats['successful_pages'] += 1\n",
    "        \n",
    "            print(f\"成功处理: {url} (找到 {len(data)} 条数据)\")\n",
    "            return next_url\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"解析错误 {url}: {e}\")\n",
    "            # 即使出错也要标记为已访问\n",
    "            if 'content_hash' in locals():\n",
    "                await self.db.mark_seen(url, \"parse_error\", content_hash)\n",
    "            else:\n",
    "                await self.db.mark_seen(url, \"parse_error\", None)\n",
    "            self.stats['failed_pages'] += 1\n",
    "            return None\n",
    "       \n",
    "    \n",
    "    async def run(self):\n",
    "        \"\"\"运行爬虫\"\"\"\n",
    "        print(f\" 开始爬取 {self.site} 站点...\")\n",
    "        self.stats['start_time'] = datetime.now()\n",
    "        \n",
    "        # 初始化数据库\n",
    "        await self.db.init_db()\n",
    "        \n",
    "        try:\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                queue = asyncio.Queue()\n",
    "                await queue.put(self.base_url)\n",
    "\n",
    "                workers = []\n",
    "\n",
    "                for i in range(self.concurrency):\n",
    "                    worker = asyncio.create_task(self.worker(f\"worker-{i}\", session, queue))\n",
    "                    workers.append(worker)\n",
    "\n",
    "                while self.stats['total_pages'] < self.max_pages and not queue.empty():\n",
    "                    await asyncio.sleep(0.1)\n",
    "\n",
    "                for worker in workers:\n",
    "                    worker.cancel()\n",
    "\n",
    "                await asyncio.gather(*workers, return_exceptions=True)\n",
    "\n",
    "        finally:\n",
    "            # 确保数据库连接关闭\n",
    "            if hasattr(self, 'db') and self.db.conn:\n",
    "                await self.db.close()\n",
    "            # 理论上这里的session会被async with自动关闭，但如果仍然有问题，可以考虑在这里显式检查和关闭\n",
    "            # if session and not session.closed:\n",
    "            #     await session.close()\n",
    "                \n",
    "        self.stats['end_time'] = datetime.now()\n",
    "        print(f\" 爬取完成！总共处理 {self.stats['total_pages']} 个页面\")\n",
    "        \n",
    "        \n",
    "        return self.collected_data, self.stats\n",
    "    \n",
    "    # 修复worker方法\n",
    "    async def worker(self, name, session, queue):\n",
    "        \"\"\"工作线程\"\"\"\n",
    "        while True:\n",
    "            try:\n",
    "                # 检查是否达到最大页面限制\n",
    "                if self.stats['total_pages'] >= self.max_pages:\n",
    "                    break\n",
    "                \n",
    "                # 获取URL（带超时）\n",
    "                try:\n",
    "                    url = await asyncio.wait_for(queue.get(), timeout=5.0)\n",
    "                except asyncio.TimeoutError:\n",
    "                    # 队列为空且超时，退出worker\n",
    "                    if queue.empty():\n",
    "                        break\n",
    "                    continue\n",
    "                \n",
    "                # 处理页面\n",
    "                next_url = await self.process_page(session, url)\n",
    "            \n",
    "                # 添加新URL到队列\n",
    "                if next_url and self.stats['total_pages'] < self.max_pages:\n",
    "                    await queue.put(next_url)\n",
    "                \n",
    "                queue.task_done()\n",
    "            \n",
    "            except asyncio.CancelledError:\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(f\"Worker {name} 错误: {e}\")\n",
    "                try:\n",
    "                    queue.task_done()\n",
    "                except:\n",
    "                    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataExporter:\n",
    "    @staticmethod\n",
    "    async def _save_stats(stats, site):\n",
    "        \"\"\"保存统计信息 - 符合项目要求\"\"\"\n",
    "        try:\n",
    "            stats_path = 'outputs/stats.json'\n",
    "            stats_to_save = stats.copy()\n",
    "        \n",
    "            # 转换所有datetime对象为字符串\n",
    "            for key, value in stats_to_save.items():\n",
    "                if isinstance(value, datetime):\n",
    "                    stats_to_save[key] = value.isoformat()\n",
    "        \n",
    "            # 计算项目要求的统计信息\n",
    "            if 'start_time' in stats_to_save and 'end_time' in stats_to_save:\n",
    "                try:\n",
    "                    # 解析时间字符串\n",
    "                    if isinstance(stats_to_save['start_time'], str):\n",
    "                        start_time = datetime.fromisoformat(stats_to_save['start_time'].replace('Z', '+00:00'))\n",
    "                    else:\n",
    "                        start_time = stats_to_save['start_time']\n",
    "                    \n",
    "                    if isinstance(stats_to_save['end_time'], str):\n",
    "                        end_time = datetime.fromisoformat(stats_to_save['end_time'].replace('Z', '+00:00'))\n",
    "                    else:\n",
    "                        end_time = stats_to_save['end_time']\n",
    "                \n",
    "                    if isinstance(start_time, datetime) and isinstance(end_time, datetime):\n",
    "                        # 总耗时\n",
    "                        total_duration = (end_time - start_time).total_seconds()\n",
    "                        stats_to_save['total_duration'] = round(total_duration, 2)\n",
    "                        \n",
    "                        # 获取其他统计值\n",
    "                        total_pages = stats.get('total_pages', 0)\n",
    "                        duplicate_pages = stats.get('duplicate_pages', 0)\n",
    "                        successful_pages = stats.get('successful_pages', 0)\n",
    "                        \n",
    "                        # 平均延迟\n",
    "                        if total_pages > 0:\n",
    "                            stats_to_save['avg_duration_per_page'] = round(total_duration / total_pages, 2)\n",
    "                        else:\n",
    "                            stats_to_save['avg_duration_per_page'] = 0\n",
    "                            \n",
    "                        # 重复比例\n",
    "                        if total_pages > 0:\n",
    "                            stats_to_save['duplicate_rate'] = round(duplicate_pages / total_pages, 3)\n",
    "                        else:\n",
    "                            stats_to_save['duplicate_rate'] = 0\n",
    "                            \n",
    "                        # 成功率\n",
    "                        if total_pages > 0:\n",
    "                            stats_to_save['success_rate'] = round(successful_pages / total_pages, 3)\n",
    "                        else:\n",
    "                            stats_to_save['success_rate'] = 0\n",
    "                except Exception as e:\n",
    "                    print(f\"计算统计信息时出错: {e}\")\n",
    "                    # 如果计算失败，设置默认值\n",
    "                    stats_to_save.update({\n",
    "                        'total_duration': 0,\n",
    "                        'avg_duration_per_page': 0,\n",
    "                        'duplicate_rate': 0,\n",
    "                        'success_rate': 0\n",
    "                    })\n",
    "        \n",
    "            # 添加站点信息\n",
    "            stats_to_save['site'] = site\n",
    "            stats_to_save['crawled_at'] = datetime.now().isoformat()\n",
    "        \n",
    "            with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(stats_to_save, f, indent=2, ensure_ascii=False)\n",
    "            print(f\" 统计信息已保存: {stats_path}\")\n",
    "            \n",
    "            return stats_to_save  # 返回计算后的统计信息\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\" 保存统计信息失败: {e}\")\n",
    "            return stats\n",
    "    \n",
    "    @staticmethod\n",
    "    async def export_data(data, stats, site):\n",
    "        \"\"\"导出数据到CSV和JSONL - 符合项目要求\"\"\"\n",
    "        # 确保outputs目录存在\n",
    "        os.makedirs('outputs', exist_ok=True)\n",
    "        \n",
    "        # 处理空数据的情况\n",
    "        if not data or len(data) == 0:\n",
    "            print(\"没有数据，创建符合要求的空文件\")\n",
    "            \n",
    "            # 根据站点类型创建正确的表头\n",
    "            if site == 'quotes':\n",
    "                columns = ['text', 'author', 'tags', 'author_url', 'page_url']\n",
    "            else:  # books\n",
    "                columns = ['title', 'price', 'stock', 'rating', 'category', 'product_url', 'page_url']\n",
    "            \n",
    "            df_empty = pd.DataFrame(columns=columns)\n",
    "            \n",
    "            # 导出CSV - UTF-8编码，字段顺序固定\n",
    "            csv_path = 'outputs/data.csv'\n",
    "            df_empty.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "            print(f\" CSV文件已创建: {csv_path}\")\n",
    "            \n",
    "            # 导出JSONL\n",
    "            jsonl_path = 'outputs/data.jsonl'\n",
    "            with open(jsonl_path, 'w', encoding='utf-8') as f:\n",
    "                pass  # 创建空文件\n",
    "            print(f\" JSONL文件已创建: {jsonl_path}\")\n",
    "            \n",
    "            # 保存统计信息并获取计算后的统计信息\n",
    "            calculated_stats = await DataExporter._save_stats(stats, site)\n",
    "            \n",
    "            return df_empty, calculated_stats  # 返回DataFrame和计算后的统计信息\n",
    "        \n",
    "        # 有数据的情况\n",
    "        try:\n",
    "            # 转换为DataFrame并确保字段顺序\n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # 确保字段顺序符合项目要求\n",
    "            if site == 'quotes':\n",
    "                expected_columns = ['text', 'author', 'tags', 'author_url', 'page_url']\n",
    "            else:  # books\n",
    "                expected_columns = ['title', 'price', 'stock', 'rating', 'category', 'product_url', 'page_url']\n",
    "            \n",
    "            # 只保留需要的列并按正确顺序排列\n",
    "            available_columns = [col for col in expected_columns if col in df.columns]\n",
    "            df = df[available_columns]\n",
    "            \n",
    "            # 导出CSV - UTF-8编码\n",
    "            csv_path = 'outputs/data.csv'\n",
    "            df.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "            print(f\" CSV文件已保存: {csv_path} ({len(df)} 条记录)\")\n",
    "            \n",
    "            # 导出JSONL\n",
    "            jsonl_path = 'outputs/data.jsonl'\n",
    "            df.to_json(jsonl_path, orient='records', lines=True, force_ascii=False)\n",
    "            print(f\" JSONL文件已保存: {jsonl_path}\")\n",
    "            \n",
    "            # 保存统计信息并获取计算后的统计信息\n",
    "            calculated_stats = await DataExporter._save_stats(stats, site)\n",
    "            \n",
    "            return df, calculated_stats\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" 保存数据文件失败: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None, stats\n",
    "\n",
    "    @staticmethod\n",
    "    async def generate_report(df, stats, site):\n",
    "        \"\"\"生成HTML报告 - 符合项目要求的可视化\"\"\"\n",
    "        # 使用传入的stats，而不是重新复制\n",
    "        stats_for_report = stats\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"没有数据生成报告\")\n",
    "            return await DataExporter._generate_basic_report(stats_for_report, site)\n",
    "    \n",
    "        try:\n",
    "            # 确保统计信息中有正确的值\n",
    "            total_pages = stats_for_report.get('total_pages', 0)\n",
    "            success_rate = stats_for_report.get('success_rate', 0) * 100\n",
    "            duplicate_rate = stats_for_report.get('duplicate_rate', 0) * 100\n",
    "            total_duration = stats_for_report.get('total_duration', 0)\n",
    "            \n",
    "            print(f\"调试信息 - 统计值: total_pages={total_pages}, success_rate={success_rate}%, duplicate_rate={duplicate_rate}%, total_duration={total_duration}s\")\n",
    "\n",
    "            # 创建包含可视化图表的HTML报告\n",
    "            html_content = [\n",
    "                \"<!DOCTYPE html>\",\n",
    "                \"<html>\",\n",
    "                \"<head>\",\n",
    "                \"    <meta charset=\\\"UTF-8\\\">\",\n",
    "                \"    <title>Async MiniCrawler Report - \" + site.capitalize() + \"</title>\",\n",
    "                \"    <script src='https://cdn.plot.ly/plotly-latest.min.js'></script>\",\n",
    "                \"    <style>\",\n",
    "                \"        body { font-family: Arial, sans-serif; margin: 40px; background: #f8f9fa; }\",\n",
    "                \"        .header { text-align: center; margin-bottom: 30px; }\",\n",
    "                \"        .kpi-container { display: flex; justify-content: center; flex-wrap: wrap; gap: 20px; margin-bottom: 40px; }\",\n",
    "                \"        .kpi-card { \",\n",
    "                \"            background: white; padding: 25px; border-radius: 12px;\",\n",
    "                \"            box-shadow: 0 2px 10px rgba(0,0,0,0.1); min-width: 200px; text-align: center;\",\n",
    "                \"        }\",\n",
    "                \"        .kpi-value { font-size: 28px; font-weight: bold; color: #007bff; margin: 10px 0; }\",\n",
    "                \"        .kpi-label { color: #6c757d; font-size: 14px; }\",\n",
    "                \"        .chart-container { background: white; padding: 20px; border-radius: 12px;\",\n",
    "                \"            box-shadow: 0 2px 10px rgba(0,0,0,0.1); margin: 20px 0; }\",\n",
    "                \"        .data-table { margin: 30px 0; }\",\n",
    "                \"        table { width: 100%; border-collapse: collapse; background: white; }\",\n",
    "                \"        th, td { padding: 12px; text-align: left; border-bottom: 1px solid #dee2e6; }\",\n",
    "                \"        th { background-color: #f8f9fa; font-weight: bold; }\",\n",
    "                \"        .positive { color: #28a745; }\",\n",
    "                \"        .negative { color: #dc3545; }\",\n",
    "                \"    </style>\",\n",
    "                \"</head>\",\n",
    "                \"<body>\",\n",
    "                \"    <div class='header'>\",\n",
    "                \"        <h1>📊 迷你爬虫报告</h1>\",\n",
    "                \"        <h2>\" + (\"名言警句\" if site == 'quotes' else \"图书\") + \"站点分析</h2>\",\n",
    "                \"        <p>生成时间: \" + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + \"</p>\",\n",
    "                \"    </div>\",\n",
    "            ]\n",
    "        \n",
    "            # KPI 概览卡片\n",
    "            html_content.extend([\n",
    "                \"    <div class='kpi-container'>\",\n",
    "                \"        <div class='kpi-card'>\",\n",
    "                \"            <div class='kpi-label'>总页面数</div>\",\n",
    "                \"            <div class='kpi-value'>\" + str(total_pages) + \"</div>\",\n",
    "                \"        </div>\",\n",
    "                \"        <div class='kpi-card'>\",\n",
    "                \"            <div class='kpi-label'>成功率</div>\",\n",
    "                \"            <div class='kpi-value'>\" + f\"{success_rate:.1f}%\" + \"</div>\",\n",
    "                \"        </div>\",\n",
    "                \"        <div class='kpi-card'>\",\n",
    "                \"            <div class='kpi-label'>重复率</div>\",\n",
    "                \"            <div class='kpi-value'>\" + f\"{duplicate_rate:.1f}%\" + \"</div>\",\n",
    "                \"        </div>\",\n",
    "                \"        <div class='kpi-card'>\",\n",
    "                \"            <div class='kpi-label'>总耗时</div>\",\n",
    "                \"            <div class='kpi-value'>\" + f\"{total_duration:.1f}s\" + \"</div>\",\n",
    "                \"        </div>\",\n",
    "                \"    </div>\",\n",
    "            ])\n",
    "        \n",
    "            # 可视化图表 - Top 10 作者/分类\n",
    "            if site == 'quotes' and 'author' in df.columns and not df.empty:\n",
    "                author_counts = df['author'].value_counts().head(10)\n",
    "                if not author_counts.empty:\n",
    "                    html_content.extend(DataExporter._create_author_chart(author_counts))\n",
    "            elif site == 'books' and 'category' in df.columns and not df.empty:\n",
    "                category_counts = df['category'].value_counts().head(10)\n",
    "                if not category_counts.empty:\n",
    "                    html_content.extend(DataExporter._create_category_chart(category_counts))\n",
    "        \n",
    "            # 数据预览\n",
    "            html_content.extend([\n",
    "                \"    <div class='chart-container'>\",\n",
    "                \"        <h3>数据预览 (前10条记录)</h3>\",\n",
    "                \"        <div class='data-table'>\" + df.head(10).to_html(classes='table table-striped', index=False, escape=False) + \"</div>\",\n",
    "                \"    </div>\",\n",
    "            ])\n",
    "        \n",
    "            # 详细统计信息\n",
    "            html_content.extend([\n",
    "                \"    <div class='chart-container'>\",\n",
    "                \"        <h3>详细统计信息</h3>\",\n",
    "                \"        <pre style='background: #f8f9fa; padding: 20px; border-radius: 5px;'>\" + \n",
    "                json.dumps(stats_for_report, indent=2, ensure_ascii=False) + \"</pre>\",\n",
    "                \"    </div>\",\n",
    "            ])\n",
    "        \n",
    "            html_content.extend([\n",
    "                \"</body>\",\n",
    "                \"</html>\"\n",
    "            ])\n",
    "        \n",
    "            # 保存报告\n",
    "            report_path = 'outputs/report.html'\n",
    "            with open(report_path, 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(html_content))\n",
    "        \n",
    "            print(f\" HTML报告已生成: {report_path}\")\n",
    "            return report_path\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\" 生成报告失败: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return await DataExporter._generate_basic_report(stats_for_report, site)\n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_author_chart(author_counts):\n",
    "        \"\"\"创建作者条形图\"\"\"\n",
    "        authors = author_counts.index.tolist()\n",
    "        counts = author_counts.values.tolist()\n",
    "        \n",
    "        return [\n",
    "            \"    <div class='chart-container'>\",\n",
    "            \"        <h3>Top 10 作者</h3>\",\n",
    "            \"        <div id='author-chart'></div>\",\n",
    "            \"        <script>\",\n",
    "            \"            var authorData = {\",\n",
    "            \"                x: \" + json.dumps(counts) + \",\",\n",
    "            \"                y: \" + json.dumps(authors) + \",\",\n",
    "            \"                type: 'bar',\",\n",
    "            \"                orientation: 'h',\",\n",
    "            \"                marker: { color: '#007bff' }\",\n",
    "            \"            };\",\n",
    "            \"            var layout = {\",\n",
    "            \"                height: 400,\",\n",
    "            \"                margin: { l: 150 },\",\n",
    "            \"                title: 'Top 10 作者'\",\n",
    "            \"            };\",\n",
    "            \"            Plotly.newPlot('author-chart', [authorData], layout);\",\n",
    "            \"        </script>\",\n",
    "            \"    </div>\"\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    def _create_category_chart(category_counts):\n",
    "        \"\"\"创建分类条形图\"\"\"\n",
    "        categories = category_counts.index.tolist()\n",
    "        counts = category_counts.values.tolist()\n",
    "        \n",
    "        return [\n",
    "            \"    <div class='chart-container'>\",\n",
    "            \"        <h3>Top 10 分类</h3>\",\n",
    "            \"        <div id='category-chart'></div>\",\n",
    "            \"        <script>\",\n",
    "            \"            var categoryData = {\",\n",
    "            \"                x: \" + json.dumps(counts) + \",\",\n",
    "            \"                y: \" + json.dumps(categories) + \",\",\n",
    "            \"                type: 'bar',\",\n",
    "            \"                orientation: 'h',\",\n",
    "            \"                marker: { color: '#28a745' }\",\n",
    "            \"            };\",\n",
    "            \"            var layout = {\",\n",
    "            \"                height: 400,\",\n",
    "            \"                margin: { l: 150 },\",\n",
    "            \"                title: 'Top 10 分类'\",\n",
    "            \"            };\",\n",
    "            \"            Plotly.newPlot('category-chart', [categoryData], layout);\",\n",
    "            \"        </script>\",\n",
    "            \"    </div>\"\n",
    "        ]\n",
    "    \n",
    "    @staticmethod\n",
    "    async def _generate_basic_report(stats, site):\n",
    "        \"\"\"生成基本报告（当没有数据时）\"\"\"\n",
    "        try:\n",
    "            # 确保stats中没有datetime对象\n",
    "            stats_for_report = stats.copy()\n",
    "            for key, value in stats_for_report.items():\n",
    "                if isinstance(value, datetime):\n",
    "                    stats_for_report[key] = value.isoformat()\n",
    "        \n",
    "            html_content = [\n",
    "                \"<!DOCTYPE html>\",\n",
    "                \"<html>\",\n",
    "                \"<head>\",\n",
    "                \"    <meta charset=\\\"UTF-8\\\">\",  # 添加UTF-8编码声明\n",
    "                \"    <title>迷你爬虫报告</title>\",\n",
    "                \"</head>\",\n",
    "                \"<body>\",\n",
    "                \"    <h1>迷你爬虫报告 - \" + (\"名言警句\" if site == 'quotes' else \"图书\") + \"</h1>\",\n",
    "                \"    <p>生成时间: \" + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + \"</p>\",\n",
    "                \"    <h2>没有收集到数据</h2>\",\n",
    "                \"    <p>爬虫已完成但没有收集到任何数据。</p>\",\n",
    "                \"    <h3>统计信息:</h3>\",\n",
    "                \"    <pre>\" + json.dumps(stats_for_report, indent=2, ensure_ascii=False) + \"</pre>\",\n",
    "                \"</body>\",\n",
    "                \"</html>\"\n",
    "            ]\n",
    "        \n",
    "            report_path = 'outputs/report.html'\n",
    "            with open(report_path, 'w', encoding='utf-8') as f:\n",
    "                f.write('\\n'.join(html_content))\n",
    "        \n",
    "            print(f\" 基本HTML报告已生成: {report_path}\")\n",
    "            return report_path\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\" 生成基本报告失败: {e}\")\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 设置测试环境...\n",
      " tests/fixtures/quotes_page1.html 存在\n",
      " tests/fixtures/books_page1.html 存在\n",
      " tests/test_crawler.py 存在\n",
      "\n",
      " 开始运行测试...\n",
      "==================================================\n",
      "\n",
      " 测试部分完成！\n"
     ]
    }
   ],
   "source": [
    "# ==================== 测试运行部分 ====================\n",
    "\n",
    "print(\" 设置测试环境...\")\n",
    "\n",
    "# 确保测试文件存在且路径正确\n",
    "import os\n",
    "\n",
    "# 检查测试文件是否存在\n",
    "test_files = [\n",
    "    'tests/fixtures/quotes_page1.html',\n",
    "    'tests/fixtures/books_page1.html',\n",
    "    'tests/test_crawler.py'\n",
    "]\n",
    "\n",
    "for file in test_files:\n",
    "    if os.path.exists(file):\n",
    "        print(f\" {file} 存在\")\n",
    "    else:\n",
    "        print(f\" {file} 不存在，请先创建测试文件\")\n",
    "\n",
    "print(\"\\n 开始运行测试...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 直接在notebook中运行测试函数\n",
    "def run_tests_in_notebook():\n",
    "    \"\"\"在Notebook中直接运行测试\"\"\"\n",
    "    \n",
    "    # 临时定义测试函数（因为import可能有问题）\n",
    "    def test_parse_quotes():\n",
    "        \"\"\"测试名言页面解析\"\"\"\n",
    "        print(\"测试名言页面解析...\")\n",
    "        \n",
    "        try:\n",
    "            with open('tests/fixtures/quotes_page1.html', 'r', encoding='utf-8') as f:\n",
    "                html = f.read()\n",
    "            \n",
    "            parser = PageParser()\n",
    "            quotes, next_url = parser.parse_quotes(html, 'http://test.com')\n",
    "            \n",
    "            # 验证解析结果\n",
    "            assert len(quotes) == 1, f\"应该解析出1条名言，实际得到: {len(quotes)}\"\n",
    "            assert quotes[0]['author'] == 'Alan Kay', f\"作者应该是Alan Kay，实际是: {quotes[0]['author']}\"\n",
    "            assert 'future' in quotes[0]['tags'], f\"应该包含future标签，实际标签: {quotes[0]['tags']}\"\n",
    "            assert 'invention' in quotes[0]['tags'], f\"应该包含invention标签，实际标签: {quotes[0]['tags']}\"\n",
    "            assert next_url == 'http://test.com/page/2/', f\"下一页URL应该是http://test.com/page/2/，实际是: {next_url}\"\n",
    "            \n",
    "            expected_text = '\\\"The best way to predict the future is to invent it.\\\"'\n",
    "            assert quotes[0]['text'] == expected_text, f\"名言文本不匹配\"\n",
    "            \n",
    "            print(\" 名言解析测试通过\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" 名言解析测试失败: {e}\")\n",
    "            return False\n",
    "\n",
    "    def test_parse_books():\n",
    "        \"\"\"测试图书页面解析\"\"\"\n",
    "        print(\"测试图书页面解析...\")\n",
    "        \n",
    "        try:\n",
    "            with open('tests/fixtures/books_page1.html', 'r', encoding='utf-8') as f:\n",
    "                html = f.read()\n",
    "            \n",
    "            parser = PageParser()\n",
    "            books, next_url = parser.parse_books(html, 'http://test.com')\n",
    "            \n",
    "            # 验证解析结果\n",
    "            assert len(books) == 1, f\"应该解析出1本图书，实际得到: {len(books)}\"\n",
    "            assert books[0]['title'] == 'A Light in the Attic', f\"书名应该是A Light in the Attic，实际是: {books[0]['title']}\"\n",
    "            assert books[0]['price'] == '£51.77', f\"价格应该是£51.77，实际是: {books[0]['price']}\"\n",
    "            assert books[0]['stock'] == 'In stock', f\"库存状态应该是In stock，实际是: {books[0]['stock']}\"\n",
    "            assert books[0]['rating'] == 'Three', f\"评分应该是Three，实际是: {books[0]['rating']}\"\n",
    "            assert next_url == 'http://test.com/page-2.html', f\"下一页URL应该是http://test.com/page-2.html，实际是: {next_url}\"\n",
    "            \n",
    "            print(\" 图书解析测试通过\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" 图书解析测试失败: {e}\")\n",
    "            return False\n",
    "\n",
    "    # 运行测试\n",
    "    tests = [test_parse_quotes, test_parse_books]\n",
    "    results = []\n",
    "    \n",
    "    for test in tests:\n",
    "        try:\n",
    "            result = test()\n",
    "            results.append(result)\n",
    "        except Exception as e:\n",
    "            print(f\" 测试运行错误: {e}\")\n",
    "            results.append(False)\n",
    "    \n",
    "    # 显示结果\n",
    "    print(\"\\\\n\" + \"=\" * 50)\n",
    "    print(\" 测试结果汇总:\")\n",
    "    print(f\"总测试数: {len(tests)}\")\n",
    "    print(f\"通过数: {sum(results)}\")\n",
    "    print(f\"失败数: {len(tests) - sum(results)}\")\n",
    "    \n",
    "    if all(results):\n",
    "        print(\" 所有测试通过！\")\n",
    "    else:\n",
    "        print(\" 有测试失败！\")\n",
    "    \n",
    "    return all(results)\n",
    "\n",
    "print(\"\\n 测试部分完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已清理outputs目录\n",
      "清理完成，准备运行爬虫\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "\n",
    "def force_close_database():\n",
    "    \"\"\"强制关闭所有可能的数据库连接\"\"\"\n",
    "    try:\n",
    "        # 尝试连接并立即关闭来释放文件锁\n",
    "        if os.path.exists('seen_urls.db'):\n",
    "            conn = sqlite3.connect('seen_urls.db')\n",
    "            conn.close()\n",
    "            print(\"数据库连接已强制关闭\")\n",
    "    except Exception as e:\n",
    "        print(f\"强制关闭数据库时出错: {e}\")\n",
    "    \n",
    "    # 尝试删除数据库文件\n",
    "    db_files = ['seen_urls.db', 'seen_urls.db-journal']\n",
    "    for db_file in db_files:\n",
    "        if os.path.exists(db_file):\n",
    "            try:\n",
    "                os.remove(db_file)\n",
    "                print(f\"已删除数据库文件: {db_file}\")\n",
    "            except Exception as e:\n",
    "                print(f\"无法删除 {db_file}: {e}\")\n",
    "# 彻底清理outputs目录和数据库文件\n",
    "# 彻底清理\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 先强制关闭数据库\n",
    "force_close_database()\n",
    "\n",
    "# 清理outputs目录\n",
    "if os.path.exists('outputs'):\n",
    "    shutil.rmtree('outputs')\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "print(\"已清理outputs目录\")\n",
    "\n",
    "print(\"清理完成，准备运行爬虫\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "清理完成\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "# 彻底删除所有旧文件\n",
    "files_to_remove = ['seen_urls.db'] + glob.glob('outputs/*')\n",
    "for file in files_to_remove:\n",
    "    if os.path.exists(file):\n",
    "        try:\n",
    "            if os.path.isfile(file):\n",
    "                os.remove(file)\n",
    "                print(f\"已删除: {file}\")\n",
    "            elif os.path.isdir(file):\n",
    "                import shutil\n",
    "                shutil.rmtree(file)\n",
    "                print(f\"已删除目录: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"无法删除 {file}: {e}\")\n",
    "\n",
    "print(\"清理完成\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据库初始化完成\n",
      "数据库测试: 成功\n",
      "数据库连接已关闭。\n"
     ]
    }
   ],
   "source": [
    "# 先测试数据库功能\n",
    "async def test_database():\n",
    "    db = URLDatabase()\n",
    "    await db.init_db()\n",
    "    \n",
    "    # 测试插入和查询\n",
    "    test_url = \"https://quotes.toscrape.com/test\"\n",
    "    await db.mark_seen(test_url, \"test\", \"test_hash\")\n",
    "    \n",
    "    is_seen = await db.is_seen(test_url)\n",
    "    print(f\"数据库测试: {'成功' if is_seen else '失败'}\")\n",
    "    \n",
    "    await db.close()\n",
    "\n",
    "await test_database()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run_crawler(site='quotes', concurrency=3, max_pages=20, delay=1.0):\n",
    "    \"\"\"运行爬虫并导出所有要求的文件\"\"\"\n",
    "    print(f\"开始爬取 {site} 站点\")\n",
    "    print(f\"配置: {concurrency} 并发, 最多 {max_pages} 页面, 延迟 {delay}s\")\n",
    "    \n",
    "    crawler = None\n",
    "    try:\n",
    "        crawler = AsyncCrawler(site, concurrency, max_pages, delay)\n",
    "        data, stats = await crawler.run()\n",
    "        \n",
    "        print(f\"爬虫完成，获得 {len(data) if data else 0} 条数据\")\n",
    "        \n",
    "        # 导出数据\n",
    "        print(\"开始导出数据文件...\")\n",
    "        df, calculated_stats = await DataExporter.export_data(data, stats, site)\n",
    "        \n",
    "        if df is not None:\n",
    "            print(f\"数据导出成功，共 {len(df)} 条记录\")\n",
    "        \n",
    "            # 生成HTML报告，使用计算后的统计信息\n",
    "            print(\"生成可视化报告...\")\n",
    "            report_path = await DataExporter.generate_report(df, calculated_stats, site)\n",
    "            \n",
    "            if report_path:\n",
    "                print(f\"所有文件生成完成！\")\n",
    "                print(f\" - CSV: outputs/data.csv\")\n",
    "                print(f\" - JSONL: outputs/data.jsonl\") \n",
    "                print(f\" - Stats: outputs/stats.json\")\n",
    "                print(f\" - Report: outputs/report.html\")\n",
    "            else:\n",
    "                print(\"报告生成失败\")\n",
    "        else:\n",
    "            print(\"数据导出失败\")\n",
    "        \n",
    "        return data, calculated_stats  # 返回计算后的统计信息\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"爬虫运行失败: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "        # 即使失败也生成统计文件\n",
    "        error_stats = {\n",
    "            'total_pages': 0,\n",
    "            'successful_pages': 0,\n",
    "            'failed_pages': 1,\n",
    "            'duplicate_pages': 0,\n",
    "            'start_time': datetime.now().isoformat(),\n",
    "            'end_time': datetime.now().isoformat(),\n",
    "            'error': str(e),\n",
    "            'status': 'failed'\n",
    "        }\n",
    "        await DataExporter.export_data([], error_stats, site)\n",
    "        \n",
    "        return [], error_stats\n",
    "    finally:\n",
    "        # 确保数据库连接关闭（安全地关闭）\n",
    "        try:\n",
    "            if crawler and hasattr(crawler, 'db') and crawler.db.conn:\n",
    "                await crawler.db.close()\n",
    "                print(\"数据库连接已安全关闭\")\n",
    "        except Exception as e:\n",
    "            print(f\"关闭数据库连接时出错: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已清理outputs目录\n",
      "开始运行爬虫...\n",
      "开始爬取 quotes 站点\n",
      "配置: 2 并发, 最多 50 页面, 延迟 1.0s\n",
      " 开始爬取 quotes 站点...\n",
      "数据库初始化完成\n",
      "成功处理: https://quotes.toscrape.com (找到 10 条数据)\n",
      "成功处理: https://quotes.toscrape.com/page/2/ (找到 10 条数据)\n",
      "成功处理: https://quotes.toscrape.com/page/3/ (找到 10 条数据)\n",
      "成功处理: https://quotes.toscrape.com/page/4/ (找到 10 条数据)\n",
      "成功处理: https://quotes.toscrape.com/page/5/ (找到 10 条数据)\n",
      "成功处理: https://quotes.toscrape.com/page/6/ (找到 10 条数据)\n",
      "成功处理: https://quotes.toscrape.com/page/7/ (找到 10 条数据)\n",
      "成功处理: https://quotes.toscrape.com/page/8/ (找到 10 条数据)\n",
      "成功处理: https://quotes.toscrape.com/page/9/ (找到 10 条数据)\n",
      "成功处理: https://quotes.toscrape.com/page/10/ (找到 10 条数据)\n",
      "数据库连接已关闭。\n",
      " 爬取完成！总共处理 10 个页面\n",
      "爬虫完成，获得 100 条数据\n",
      "开始导出数据文件...\n",
      " CSV文件已保存: outputs/data.csv (100 条记录)\n",
      " JSONL文件已保存: outputs/data.jsonl\n",
      " 统计信息已保存: outputs/stats.json\n",
      "数据导出成功，共 100 条记录\n",
      "生成可视化报告...\n",
      "调试信息 - 统计值: total_pages=10, success_rate=100.0%, duplicate_rate=0.0%, total_duration=19.31s\n",
      " HTML报告已生成: outputs/report.html\n",
      "所有文件生成完成！\n",
      " - CSV: outputs/data.csv\n",
      " - JSONL: outputs/data.jsonl\n",
      " - Stats: outputs/stats.json\n",
      " - Report: outputs/report.html\n",
      "关闭数据库连接时出错: no active connection\n",
      "\n",
      "=== 文件生成情况 ===\n",
      " outputs\\data.csv - 25852 字节\n",
      "  数据行数: 100\n",
      "  列名: ['text', 'author', 'tags', 'author_url', 'page_url']\n",
      " outputs\\data.jsonl - 31919 字节\n",
      " outputs\\report.html - 7080 字节\n",
      " outputs\\stats.json - 371 字节\n",
      "  统计信息: 10 页面\n"
     ]
    }
   ],
   "source": [
    "# 清理outputs目录\n",
    "import shutil\n",
    "if os.path.exists('outputs'):\n",
    "    shutil.rmtree('outputs')\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "print(\"已清理outputs目录\")\n",
    "\n",
    "# 运行爬虫\n",
    "print(\"开始运行爬虫...\")\n",
    "try:\n",
    "    data, stats = await run_crawler('quotes', concurrency=2, max_pages=50, delay=1.0)\n",
    "    \n",
    "    # 检查所有生成的文件\n",
    "    print(\"\\n=== 文件生成情况 ===\")\n",
    "    import glob\n",
    "    files = glob.glob('outputs/*')\n",
    "    \n",
    "    if not files:\n",
    "        print(\" 没有生成任何文件\")\n",
    "    else:\n",
    "        for file in files:\n",
    "            if os.path.exists(file):\n",
    "                file_size = os.path.getsize(file)\n",
    "                print(f\" {file} - {file_size} 字节\")\n",
    "                \n",
    "                # 显示文件内容预览\n",
    "                if file.endswith('.csv') and file_size > 0:\n",
    "                    try:\n",
    "                        df_check = pd.read_csv(file)\n",
    "                        print(f\"  数据行数: {len(df_check)}\")\n",
    "                        print(f\"  列名: {list(df_check.columns)}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  读取CSV文件失败: {e}\")\n",
    "                \n",
    "                elif file.endswith('.json') and file_size > 0:\n",
    "                    try:\n",
    "                        with open(file, 'r', encoding='utf-8') as f:\n",
    "                            stats_content = json.load(f)\n",
    "                        print(f\"  统计信息: {stats_content.get('total_pages', '未知')} 页面\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"  读取JSON文件失败: {e}\")\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"爬虫运行失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
